name: "reverse_experiment"

data:
    src: "src"
    trg: "trg"
    # generate data with scripts/generate_reverse_task.py
    train: "data/reverse/train"
    dev: "data/reverse/dev"
    test: "data/reverse/test"
    level: "word"
    lowercase: False
    max_sent_length: 25
    src_voc_min_freq: 0
    src_voc_limit: 100
    trg_voc_min_freq: 0
    trg_voc_limit: 100
    #src_vocab: "reverse_model/src_vocab.txt"
    #trg_vocab: "reverse_model/trg_vocab.txt"

testing:
    beam_size: 1
    alpha: 1.0

training:
    random_seed: 42
    step_opt_type: "adam"
    step_opt_params:
        start: 0.005
    batch_type: "sentence"
    max_output_length: 30
    eval_metric: "sequence_accuracy"
    use_cuda: False
    model_dir: "test/r.ad.005"
    validation_freq: 1
    batch_size: 256
    sigma:
        start: 0.05
    start: 'data/reverse/checkpoints32/2000.ckpt'
    #emb_start: 'data/reverse/checkpoints32/9400.ckpt'
    num_expl_directions: 50
    #use_only_best: 24
    #normalizer: 'stdev'
    #grad_estimator: "forward"
    #num_workers: 2
    #parallel_func_evals: False
    #structured: False
    iterations: 10

model:
    initializer: "xavier"
    embed_initializer: "normal"
    embed_init_weight: 0.1
    bias_initializer: "zeros"
    init_rnn_orthogonal: False
    lstm_forget_gate: 0.
    encoder:
        rnn_type: "lstm"
        embeddings:
            embedding_dim: 16
            scale: False
        hidden_size: 32
        bidirectional: True
        dropout: 0.2
        num_layers: 1
    decoder:
        rnn_type: "lstm"
        embeddings:
            embedding_dim: 16
            scale: False
        hidden_size: 32
        dropout: 0.2
        hidden_dropout: 0.1
        num_layers: 1
        input_feeding: True
        init_hidden: "zero"
        attention: "luong"
