name: "reverse_experiment"

data:  # legacy from JoeyNMT
    src: "src"
    trg: "trg"

    # paths to the data
    train: "data/reverse/train"     # change middle part to change task - e.g. "data/copy/train"
    dev: "data/reverse/dev"
    test: "data/reverse/test"
    level: "word"
    lowercase: False
    max_sent_length: 25
    src_voc_min_freq: 0
    src_voc_limit: 100
    trg_voc_min_freq: 0
    trg_voc_limit: 100


training:  # settings for DFO training

    random_seed: 42   # relevant for minibatching and noise generation

    step_opt_type: "sgd" # the optimizer; optional, default "sgd" (choices: "adam", "momentum", "sgd")

    # hyperparams for "adam", remove if using another optimizer
    step_opt_params:
        start: 0.001   # starting learning rate; required
        beta1: 0.9     # optional, default 0.9
        beta2: 0.999   # optional, default 0.999
        epsilon: 1e-08      # optional, default 1e-08

    # hyperparams for "momentum", remove if using another optimizer
    step_opt_params:
        start: 0.001   # starting learning rate; required
        momentum: 0.9  # optional, default 0.9

    # hyperparams for "sgd", remove if using another optimizer
    step_opt_params:
        start: 0.001   # starting learning rate; required
        decay: 1.0     # optional, default 1.0 (no decay)
        min: 0.0001    # optional, default 0.0001, only relevant if decay < 1.0

    # the reward function; optional, default "bleu" (choices "bleu", "chrf", "token_accuracy", "sequence_accuracy")
    eval_metric: "bleu" 

    # whether you are training on GPU; optional, default False
    use_cuda: False

    # output folder; required, can be arbitrarily deep
    model_dir: "path/to/model/outputs"

    # frequency of evaluating the model on the validation set; required
    validation_freq: 50

    batch_size: 256   # required

    # smoothing parameter
    sigma:
        start: 0.05   # starting point; required
        decay: 1.0    # decay rate; optional, default 1.0 (no decay)
        min: 0.01     # minimum sigma value; optional, default 0.01 (only relevant when decay is used)

    # path to a checkpoint; delete for random weights initialization
    start: 'data/reverse/checkpoints32/2000.ckpt'      # change second part to change tasks, e.g. data/sort/checkpoints32/nnnn.ckpt

    # path to a(nother) checkpoint; uncomment to use pre-trained embeddings
    # emb_start: 'path/to/checkpoint'

    num_expl_directions: 50   # required
    iterations: 500   # required

    # the gradient estimator; optional, default "antithetic" (choices "vanilla", "forward", "antithetic")
    grad_estimator: "antithetic"

    # how to scale the gradient - "sigma" or "stdev" a la Mania et al. (2018)
    normalizer: "sigma"   # optional, default "sigma"

    # uncomment to use only the n top performing exploration directions
    # use_only_best: 24

    # sets the type of parallelization; if True, parallelize function evaluations, else parallelize iterations.
    # parallel_func_evals: True

    # either 10 parallel function evaluations, or 10 parallel iterations, depending on whether parallel_func_evals is set.
    # num_workers: 10

    # parallel_func_evals: True

    # uncomment to force orthogonality of the exploration directions a la Choromanski et al. (2018)
    # structured: True

    # JoeyNMT legacy
    batch_type: "sentence"  # required (choices "word", "sentence")
    max_output_length: 30   # required



model:  # the model architecture, JoeyNMT default for the reverse task, with a few exceptions
    initializer: "xavier"
    embed_initializer: "normal"
    embed_init_weight: 0.1
    bias_initializer: "zeros"
    init_rnn_orthogonal: False
    lstm_forget_gate: 0.
    encoder:
        rnn_type: "lstm"
        embeddings:
            embedding_dim: 16
            scale: False
        hidden_size: 32    # changed, was 64
        bidirectional: True
        dropout: 0.2
        num_layers: 1
    decoder:
        rnn_type: "lstm"
        embeddings:
            embedding_dim: 16
            scale: False
        hidden_size: 32    # changed, was 64
        dropout: 0.2
        hidden_dropout: 0.1
        num_layers: 1
        input_feeding: True
        init_hidden: "zero"
        attention: "luong"
